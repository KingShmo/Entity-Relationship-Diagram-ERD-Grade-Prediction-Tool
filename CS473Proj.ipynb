{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Object Recognition: YoloV8"],"metadata":{"id":"HfX2-7Tr-MB4"}},{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_XWyDGJN_Ds","executionInfo":{"status":"ok","timestamp":1725809219256,"user_tz":240,"elapsed":190,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"4aed5ac4-41a7-49e5-a217-0eefb9ef7950"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!pip install ultralytics==8.0.196\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQ1XIENVOEBW","executionInfo":{"status":"ok","timestamp":1725809238521,"user_tz":240,"elapsed":17146,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"22681a36-3840-4058-8865-ab2536817a32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.196 🚀 Python-3.10.12 torch-2.4.0+cu121 CPU (Intel Xeon 2.20GHz)\n","Setup complete ✅ (2 CPUs, 12.7 GB RAM, 32.8/107.7 GB disk)\n"]}]},{"cell_type":"code","source":["!pip install easyocr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTMdiw_OLRc9","executionInfo":{"status":"ok","timestamp":1725809251808,"user_tz":240,"elapsed":10778,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"ecfd25ac-48a4-4a40-a81a-e7dccaf82135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting easyocr\n","  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.28)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n"]}]},{"cell_type":"code","source":["!mkdir {HOME}/datasets\n","%cd {HOME}/datasets\n","\n","!pip install roboflow --quiet\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"Gl773kBPkL1O8pZYCp2y\")\n","project = rf.workspace(\"cs473-jxlk7\").project(\"cs473-sbytd\")\n","version = project.version(3)\n","dataset = version.download(\"yolov8\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vB7UTEgOHQl","executionInfo":{"status":"ok","timestamp":1725809272069,"user_tz":240,"elapsed":9233,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"1dc7ba79-0a45-4134-ee30-466b554cb530"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in CS473-3 to yolov8:: 100%|██████████| 4813/4813 [00:00<00:00, 27202.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to CS473-3 in yolov8:: 100%|██████████| 275/275 [00:00<00:00, 4997.24it/s]\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","\n","!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=50 imgsz=800 plots=True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4O-ONpZOVvA","executionInfo":{"status":"ok","timestamp":1725809628452,"user_tz":240,"elapsed":153186,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"4630a786-3bdc-43f0-f3fa-49870a3c5aa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(file, map_location='cpu'), file  # load\n","New https://pypi.org/project/ultralytics/8.2.90 available 😃 Update with 'pip install -U ultralytics'\n","Ultralytics YOLOv8.0.196 🚀 Python-3.10.12 torch-2.4.0+cu121 CPU (Intel Xeon 2.20GHz)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/datasets/CS473-3/data.yaml, epochs=50, patience=50, batch=16, imgsz=800, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n","2024-09-08 15:31:22.932586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-09-08 15:31:22.969398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-09-08 15:31:22.979300: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Overriding model.yaml nc=80 with nc=5\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n","  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n","  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n","  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n"," 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n","Model summary: 225 layers, 11137535 parameters, 11137519 gradients, 28.7 GFLOPs\n","\n","Transferred 349/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:238: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = amp.GradScaler(enabled=self.amp)\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/CS473-3/train/labels.cache... 113 images, 0 backgrounds, 0 corrupt: 100% 113/113 [00:00<?, ?it/s]\n","/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py:161: UserWarning: Got processor for bboxes, but no transform to process it.\n","  self._set_keys()\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/CS473-3/valid/labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100% 20/20 [00:00<?, ?it/s]\n","Plotting labels to runs/detect/train2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 800 train, 800 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       1/50         0G      1.207      3.528      1.245        295        800:  12% 1/8 [01:28<10:22, 88.98s/it]^C\n"]}]},{"source":["%cd {HOME}\n","\n","from ultralytics import YOLO\n","import os\n","import easyocr\n","import cv2\n","from PIL import Image\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Load YOLO model and EasyOCR\n","model = YOLO('runs/detect/train5/weights/best.pt')\n","reader = easyocr.Reader(['en'])\n","\n","# Path to images\n","images_path = '/content/drive/My Drive/CS473/OD_OCR_testing/images'\n","\n","# Create output directory if it doesn't exist\n","output_dir = f\"{HOME}/text\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Get list of image files\n","image_files = [f for f in os.listdir(images_path) if f.endswith(('.png', '.jpg', '.jpeg'))] # Filter out non-image files\n","print(image_files)\n","\n","# Loop through images\n","for image_file in image_files:\n","    image_path = f'{images_path}/{image_file}'\n","    image = cv2.imread(image_path)\n","    results = model.predict(source=image_path, conf=0.5, iou=0.4, save=True)\n","\n","    # Initialize text output for this image\n","    output_text = \"\"\n","\n","    for result_idx, result in enumerate(results):\n","        boxes = result.boxes  # Get bounding boxes\n","        class_ids = boxes.cls  # Class IDs\n","        xyxy = boxes.xyxy  # Bounding box coordinates\n","\n","        for i, box in enumerate(boxes):\n","            class_id = int(class_ids[i])  # Convert to integer for the class ID\n","            # Here you should map the class ID to the class name, assuming you have a list of class names\n","            class_name = model.names[class_id]  # Assuming YOLO model has class names\n","\n","            # Prepare output format with the class name as the first word\n","            output_text += f\"['{class_name}', \"\n","\n","            x1, y1, x2, y2 = [int(coord) for coord in xyxy[i]]\n","\n","            # Crop the detected region from the image\n","            cropped_image = image[y1:y2, x1:x2]\n","\n","            # Convert the cropped image to RGB (if necessary)\n","            cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n","\n","            # Perform OCR on the cropped region\n","            ocr_result = reader.readtext(cropped_image_rgb)\n","\n","            # Add OCR results to output text in the specified format\n","            if ocr_result:\n","                for ocr_data in ocr_result:\n","                    text = ocr_data[1]  # Detected text\n","                    output_text += f\"'{text}', \"\n","\n","                # Replace last comma with closing bracket\n","                output_text = output_text.rstrip(', ') + \"]\\n\"\n","            else:\n","                # If no text detected, close the bracket\n","                output_text = output_text.rstrip(', ') + \"no text found]\\n\"\n","\n","    # Extract the base name of the image (without extension)\n","    image_name = os.path.splitext(image_file)[0]\n","\n","    # Save the results to a .txt file with the same base name as the image in the {HOME}/text folder\n","    with open(f'{output_dir}/{image_name}.txt', 'w') as f:\n","        f.write(output_text)\n","\n","    print(f\"OCR results saved to {output_dir}/{image_name}.txt\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBMCZMskZgT2","executionInfo":{"status":"ok","timestamp":1725756109785,"user_tz":240,"elapsed":15302,"user":{"displayName":"Ryan Sevidal","userId":"02726488403055156539"}},"outputId":"672c1b02-6fbc-42dc-99d5-7a708dc8d281"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","['1.png', '2.png', '4.png', '3.png', '7.png', '8.png', '5.png', '6.png']\n"]},{"output_type":"stream","name":"stderr","text":["\n","image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/1.png: 256x800 3 entitys, 2 rels, 7 rel_attrs, 12.8ms\n","Speed: 3.1ms preprocess, 12.8ms inference, 2.4ms postprocess per image at shape (1, 3, 256, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/1.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/2.png: 736x800 3 entitys, 1 ident_rel, 4 rels, 1 weak_entity, 12.9ms\n","Speed: 4.8ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 736, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/2.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/4.png: 800x800 3 entitys, 6 ident_rels, 2 rels, 3 rel_attrs, 3 weak_entitys, 12.6ms\n","Speed: 7.6ms preprocess, 12.6ms inference, 2.3ms postprocess per image at shape (1, 3, 800, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/4.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/3.png: 672x800 3 entitys, 1 ident_rel, 3 rels, 9 rel_attrs, 2 weak_entitys, 10.8ms\n","Speed: 4.5ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 672, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/3.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/7.png: 768x800 4 entitys, 1 ident_rel, 4 rels, 1 weak_entity, 11.5ms\n","Speed: 5.0ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 768, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/7.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/8.png: 480x800 4 entitys, 1 ident_rel, 3 rels, 8.4ms\n","Speed: 3.8ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n","\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/8.txt\n"]},{"output_type":"stream","name":"stderr","text":["image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/5.png: 800x800 3 entitys, 2 ident_rels, 3 rels, 2 weak_entitys, 12.0ms\n","Speed: 4.9ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 800, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/5.txt\n"]},{"output_type":"stream","name":"stderr","text":["\n","image 1/1 /content/drive/My Drive/CS473/OD_OCR_testing/images/6.png: 416x800 5 entitys, 5 rels, 10.1ms\n","Speed: 3.8ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 800)\n","Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["OCR results saved to /content/text/6.txt\n"]}]}]}